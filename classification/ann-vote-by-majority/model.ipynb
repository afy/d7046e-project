{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN (Vote by majority)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path already added to sys.path\n"
     ]
    }
   ],
   "source": [
    "# Import dataset tools\n",
    "import sys, os\n",
    "dataset_import_path = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath('')), ''))\n",
    "if dataset_import_path not in sys.path:\n",
    "    sys.path.append(dataset_import_path)\n",
    "else:\n",
    "    print(\"Dataset path already added to sys.path\")\n",
    "\n",
    "import CloudClassificationDataset \n",
    "\n",
    "# Misc imports\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "batch_size = 3\n",
    "hidden_layer_range = [1, 5]\n",
    "hidden_layer_sizes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[0.40160936, 0.41623995, 0.3343087 ],\n",
      "        [0.50694954, 0.5084126 , 0.44111192],\n",
      "        [0.51280177, 0.5625457 , 0.53913677],\n",
      "        ...,\n",
      "        [0.6752012 , 0.7322604 , 0.569861  ],\n",
      "        [0.68983173, 0.7088515 , 0.49231896],\n",
      "        [0.6942209 , 0.670812  , 0.4703731 ]],\n",
      "\n",
      "       [[0.35479152, 0.38697878, 0.31675202],\n",
      "        [0.5508413 , 0.5084126 , 0.44257498],\n",
      "        [0.60936356, 0.59619606, 0.58010244],\n",
      "        ...,\n",
      "        [0.60643744, 0.6488661 , 0.6005852 ],\n",
      "        [0.6371617 , 0.6649598 , 0.53913677],\n",
      "        [0.64447695, 0.6942209 , 0.49524507]],\n",
      "\n",
      "       [[0.3621068 , 0.4659839 , 0.34308705],\n",
      "        [0.6079005 , 0.62838334, 0.4864667 ],\n",
      "        [0.6883687 , 0.67666423, 0.59326994],\n",
      "        ...,\n",
      "        [0.59619606, 0.6371617 , 0.56108266],\n",
      "        [0.62106806, 0.64740306, 0.5464521 ],\n",
      "        [0.6649598 , 0.66934896, 0.5596196 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.8112655 , 0.7995611 , 0.7190929 ],\n",
      "        [0.7834675 , 0.8200439 , 0.70738846],\n",
      "        [0.77615213, 0.7820044 , 0.6005852 ],\n",
      "        ...,\n",
      "        [0.61960495, 0.4835406 , 0.20409656],\n",
      "        [0.5274323 , 0.3767374 , 0.14630578],\n",
      "        [0.4381858 , 0.2962692 , 0.12948062]],\n",
      "\n",
      "       [[0.7922458 , 0.7995611 , 0.7029993 ],\n",
      "        [0.8112655 , 0.79078275, 0.7059254 ],\n",
      "        [0.78785664, 0.7644477 , 0.6356986 ],\n",
      "        ...,\n",
      "        [0.5537674 , 0.4352597 , 0.17556694],\n",
      "        [0.49231896, 0.35625458, 0.10168251],\n",
      "        [0.47183615, 0.3065106 , 0.05632773]],\n",
      "\n",
      "       [[0.7322604 , 0.79370886, 0.66934896],\n",
      "        [0.796635  , 0.7644477 , 0.6561814 ],\n",
      "        [0.7512802 , 0.69861007, 0.5888808 ],\n",
      "        ...,\n",
      "        [0.46013168, 0.37381127, 0.14630578],\n",
      "        [0.44696414, 0.37088516, 0.1038771 ],\n",
      "        [0.45720556, 0.2889539 , 0.08046818]]], dtype=float32), 1)\n"
     ]
    }
   ],
   "source": [
    "# Data setup\n",
    "\n",
    "dataset = CloudClassificationDataset.CloudClassificationDataset(\n",
    "    \"..\\\\data\\\\2A-netcdfs-cropped-from-nuria\",\n",
    "    \"..\\\\data\\\\skogs_json_train.npy\",\n",
    "    channels=(\"b04\",\"b03\",\"b02\")\n",
    ")\n",
    "validation_dataset, training_dataset = torch.utils.data.random_split(dataset,[0.2, 0.8])\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "input_size = 0\n",
    "output_size = 0\n",
    "\n",
    "for x in training_dataset:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-models setup\n",
    "\n",
    "models = {}\n",
    "\n",
    "def createAndTrainModel():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
